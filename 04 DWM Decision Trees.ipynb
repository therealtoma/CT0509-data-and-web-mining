{"cells":[{"cell_type":"markdown","metadata":{"id":"4ybs-Co-HCWj"},"source":["# Decision Trees\n"]},{"cell_type":"markdown","metadata":{"id":"d92vb77QHCWl"},"source":["# A non linearly separable dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eV_qQWeKHCWm"},"outputs":[],"source":["%matplotlib inline\n","\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_wqlHWtXHCWm","scrolled":false},"outputs":[],"source":["from sklearn.datasets import make_blobs\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","\n","# create a (quasi)random dataset\n","N_SAMPLES = 1000\n","N_CENTERS = 4\n","X, y = make_blobs(n_samples=N_SAMPLES,\n","                  centers=[[0,0], [0,1], [1,0], [1,1]],\n","                  cluster_std=0.15, random_state=4)\n","y[y==3]=0\n","y[y==2]=1\n","\n","# plot\n","plt.figure(figsize=(6,6))\n","plt.scatter(X[:,0], X[:,1], c=y, alpha=0.3);"]},{"cell_type":"markdown","metadata":{"id":"Xac-HGXMHCWn"},"source":["## What's a decision tree\n","\n"," - see see: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fic3hO11HCWn"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","model = DecisionTreeClassifier()\n","model.fit(X,y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uC9RV4U_HCWn"},"outputs":[],"source":["from sklearn.tree import plot_tree\n","\n","f_names = [\"Feature 1\", \"Feature 2\"]\n","c_names = [\"Class 0\",\"Class 1\"]\n","\n","fig, ax = plt.subplots(figsize=(20,10))\n","plot_tree(model, ax=ax,\n","         feature_names=f_names, class_names=c_names,\n","                                filled=True, rounded=True);"]},{"cell_type":"markdown","metadata":{"id":"LCAi87QQHCWo"},"source":["# Wine dataset\n","\n","Url: http://archive.ics.uci.edu/ml/datasets/Wine?ref=datanews.io\n","\n","\n","These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.\n","\n","\n","The attributes are:\n","\n","0. Quality (1-3)\n","1. Alcohol\n","1. Malic acid\n","1. Ash\n","1. Alcalinity of ash\n","1. Magnesium\n","1. Total phenols\n","1. Flavanoids\n","1. Nonflavanoid phenols\n","1. Proanthocyanins\n","1. Color intensity\n","1. Hue\n","1. OD280/OD315 of diluted wines\n","1. Proline\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SMsFOYnyHCWo"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","data_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data'\n","df = pd.read_csv(data_url, header=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ocZ-P86wHCWo"},"outputs":[],"source":["# data preparation\n","# convert to float to have precise and homogenoues computation\n","dataset = df.astype(float)\n","print(\"dataset shape\", dataset.shape)\n","\n","# get features by removing id and class\n","# remove id\n","X = dataset.loc[:,1:]\n","print(\"X shape\", X.shape)\n","\n","# get class label\n","y = dataset.loc[:,0]\n","print(\"y shape\", y.shape)"]},{"cell_type":"markdown","metadata":{"id":"HirB2rWVHCWp"},"source":["## Let's build a  tree"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IFIBuRKrHCWp"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","model = DecisionTreeClassifier()\n","model.fit(X,y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"evaEAhQGHCWp"},"outputs":[],"source":["from sklearn.tree import plot_tree\n","\n","f_names = [\"Alcohol\",\"Malic acid\", \"Ash\", \"Alcalinity\", \"Magnesium\", \"Phenols\",\n","            \"Flavanoids\", \"Nonflavanoid\", \"Proanthocyanins\", \"Color\", \"Hue\", \"ODs\", \"Proline\"]\n","\n","c_names = [\"Quality 1\",\"Quality 2\",\"Quality 3\"]\n","\n","fig, ax = plt.subplots(figsize=(20,10))\n","plot_tree(model, ax=ax,\n","         feature_names=f_names, class_names=c_names,\n","                                filled=True, rounded=True);"]},{"cell_type":"markdown","metadata":{"id":"wItObGUPHCWp"},"source":["## Decision Trees\n","\n","- Recursive Algorithm\n","- Select the best split and partition the dataset\n","- Partitioning Scenarios: **k-ary tree, binary tree, categorical, ordinal, numerical**\n","- Recursion ends when a node is pure or no further splitting is possible\n","  - other constraints can be enforced\n","\n","## Let's focus on *binary* decision tree for *classification*"]},{"cell_type":"markdown","metadata":{"id":"CaBfHMApHCWp"},"source":["## Algorithm Sketch\n","\n","\n","<div class=\"alert alert-info\">\n","\n","**BuildTree**(${\\cal D}$):\n","\n","1. *BestSplit*, *BestGain* = *None*\n","1. **For each** feature $f$\n","1. $\\quad$ **For each** threshold $t$\n","1. $\\quad\\quad$ *Gain* $\\gets$ goodness of the split $(f \\leq t)$\n","1. $\\quad\\quad$ **If** Gain>=BestGain:\n","1. $\\quad\\quad\\quad$ *BestGain* $\\gets$ *Gain*\n","1. $\\quad\\quad\\quad$ BestSplit* $\\gets$ $(f \\leq t)$\n","1. **If** *BestGain*$=0$ or *other stopping criterion is met*:\n","1. $\\quad$ $\\mu \\gets$ the best prediction for ${\\cal D}$\n","1. $\\quad$ **Return** $Leaf(\\mu)$\n","1. Let $f$ and $t$ be those of BestSplit = $(f \\leq t)$\n","1. ${\\cal D}_L \\gets \\{x \\in {\\cal D} ~|~ x_f\\leq t\\}$ *(Left Partition)*\n","1. $L \\gets$  **BuildTree**(${\\cal D}_L$) *(Left Child)*\n","1. ${\\cal D}_R \\gets \\{x \\in {\\cal D} ~|~ x_f > t\\}$ *(Right Partition)*\n","1. $R \\gets$  **BuildTree**(${\\cal D}_R$) *(Right Child)*\n","1. **Return** $Node(L,R)$\n","</div>\n","\n","\n","It is a greedy algorithm (without backtracking, i.e., decisions are not changed) that maximizes the Gain at every step."]},{"cell_type":"markdown","metadata":{"id":"ySGQPcGiHCWq"},"source":["## Driving Factor\n","\n","We let the design of our algorithm be driven by the quality measure adopted.\n","\n","For classification, we adopt **error** $E$, that is the fraction of misclassified instances."]},{"cell_type":"markdown","metadata":{"id":"KfJ8bVUpHCWq"},"source":["## Leaf Node\n","\n","- Given a dataset ${\\cal D}$ what is the best prediction we can have?\n","\n","$$\\mu = \\arg\\min\\limits_{\\mu} Error({\\cal D}, \\mu)= \\arg\\min\\limits_{\\mu} \\frac{1}{|{\\cal D}|} \\sum\\limits_{(x,y) \\in {\\cal D}} E(y, \\mu)$$\n","   - where $ E(y, \\mu)$ is 0 if $\\mu=y$ and 1 otherwise\n","\n","- for the classification task, it holds that $\\mu$ must be the most frequent label in ${\\cal D}$\n","\n","- if we denote with $p_i$ the frequency of label $l_i$ in ${\\cal D}$, we can write that total error\n","on the dataset is:\n","$$\n","Error({\\cal D}) = 1 - \\max_i p_i\n","$$\n","  - Maximum: $(1 - 1/m)$, where $m$ is the number of classes, when records are equally distributed among all classes, implying least interesting information\n","  - Minimum: (0.0) when all records belong to one class, implying most interesting information (*pure leaf*)\n","\n","- Hereinafter we denote with $Error({\\cal D})$ the error of the best prediction $\\mu$ for dataset ${\\cal D}$.\n"]},{"cell_type":"markdown","metadata":{"id":"4VkFnU9oHCWq"},"source":["## Internal Node\n","\n","- Given the pair $f \\leq t$, we must determine the quality of this split.\n","\n","- In general, assuming $Error$ is an average measure, we denote the gain of a split as the error reduction with respect to not splitting the node.\n","\n","$$\n","Gain(f,t~|~{\\cal D}) = Error({\\cal D}) - \\left(\\frac{|{\\cal D}_L|}{|{\\cal D}|} Error({\\cal D}_L) + \\frac{|{\\cal D}_R|}{|{\\cal D}|} Error({\\cal D}_R) \\right)\n","$$\n","\n","- We would like Gain>0, note that Gain cannot decrease.\n"]},{"cell_type":"markdown","metadata":{"id":"ldohysPkHCWq"},"source":["## Let's make an example\n","\n","| ${\\cal D}$ | |\n","|-|-|\n","|Class 0 | Class 1 |\n","| 400    |  400    |\n","\n","| Splitting A | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| vs. | Splitting B | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|\n","|-|-|-|-|-|-|-|-|-|\n","| ${\\cal D}_L$ | | ${\\cal D}_R$ | |     | ${\\cal D}_L$ | | ${\\cal D}_R$ | |\n","|Class 0 |  Class 1 |Class 0 | Class 1 | |Class 0 | Class 1 |Class 0 | Class 1 |\n","| 300    |  100    | 100    |  300    | | 200    |  400    | 200    |  0    |\n","| $p_0=3/4$ | ${p_1=1/4}$ | $p_0=1/4$ | $p_1=3/4$ | | $p_0= 1/3$ | $p_1= 2/3$ | $p_0= 1$ | $p_1= 0$ |\n","\n","\n","\n","\n","- Suppose $|{\\cal D}|$ has 400 instances in class 0 and 400 instances in class 1, denoted with ${\\cal D}=(400,400)$\n","   - $Error({\\cal D}) = 0.5$\n","   \n","\n","- Suppose Splitting $A=(f_1, t_1)$ produces ${\\cal D}_L=(300,100)$ and ${\\cal D}_R=(100,300)$:\n","   - $Gain(A|~{\\cal D}) = 0.5 - 400/800 * (1- 3/4) - 400/800 *(1-3/4) = 0.25$\n","   \n","   \n","- Suppose Splitting $B=(f_2, t_2)$ produces ${\\cal D}_L=(200,400)$ and ${\\cal D}_R=(200,000)$:\n","   - $Gain(B|~{\\cal D}) = 0.5 - 600/800 * (1- 2/3) - 200/800 *(1-1) = 0.25$\n","   \n","   \n","- The two splits are equally good, one of them is picked at random\n","\n","\n","\n","- Indeed, we would like to consistently prefer $B$ as it produces one *pure* child, i.e., a set of instances with perfect prediction that needs not to be processed recursively."]},{"cell_type":"markdown","metadata":{"id":"IQUxEFh2HCWq"},"source":["## Information Gain\n","### ( ID3 - Iterative Dichotomiser )\n","\n","\n","The error of a dataset is measured as the entropy of its labels distributions\n","\n","$$\n","Error({\\cal D}) = Info({\\cal D}) = -\\sum\\limits_i p_i\\log_2 ( p_i )\n","$$\n","   - where $p_i$ is the probability/frequency of label $i$\n","   - This is indeed the **entropy**, i.e., a measure of the randomness of the labels\n","   - Maximum: $\\log m$, where $m$ is the number of classes, when records are equally distributed among all classes implying least information\n","   - Minimum: 0.0 when all records belong to one class, implying most information (assume $0\\log 0=0$)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-dTD0s7lHCWq"},"source":["## Let's make an example\n","\n","- Suppose $|{\\cal D}|$ has 400 instances in class 0 and 400 instances in class 1, denote with ${\\cal D}=(400,400)$\n","   - $Error({\\cal D}) = - 1/2 \\log(1/2) - 1/2 \\log(1/2) = \\log(2) = 1$\n","   \n","\n","- Suppose Splitting $A=(f_1, t_1)$ produces ${\\cal D}_L=(300,100)$ and ${\\cal D}_R=(100,300)$:\n","   - $Gain(A|~{\\cal D}) = 1 - 400/800 * ( - 3/4 \\log(3/4) - 1/4 \\log(1/4)) - 400/800 *( - 1/4 \\log(1/4) - 3/4 \\log(3/4)) \\approx 0.19$\n","   \n","   \n","- Suppose Splitting $B=(f_2, t_2)$ produces ${\\cal D}_L=(200,400)$ and ${\\cal D}_R=(200,000)$:\n","   - $Gain(B|~{\\cal D}) = 1 - 600/800 * (- 1/3 \\log(1/3) -2/3 \\log(2/3)) - 200/800 *(- 1 \\log(1) -0 \\log(0)) \\approx 0.31$\n","   \n","   \n","- Largest Gain is for split B!"]},{"cell_type":"markdown","metadata":{"id":"ROUN_oZWHCWr"},"source":["## Gain Ratio\n","\n","### ( C4.5 Algorithm )\n","\n","- For k-ary decision trees (instead of binary), Information Gain favors splits with several small partitions\n","  - they are more likely to be pure\n","  \n","\n","- Gain Ration normalizes the Information Gain by the SplitInfo of a k-way splitting:\n","$$\n","SplitInfo({\\cal D}) =  -\\sum\\limits_{i=1}^k \\frac{|{\\cal D}_i|}{|{\\cal D}|} \\log \\left(\\frac{|{\\cal D}_i|}{|{\\cal D}|}\\right)\n","$$\n","  - this is analogous to the information gain, but related to the partitioning instead of the labels\n","  - large values of $k$ (complex partitionings) receive a large score\n","  \n","  \n","  \n","- The Gain Ratio is formalized as:\n","$$\n","Error({\\cal D}) = GainRatio({\\cal D}) = \\frac{Info({\\cal D})}{SplitInfo({\\cal D})}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"mP07DJjSHCWr"},"source":["## GINI Index\n","\n","### ( CART - Classification and Regression Trees)\n","\n","GINI is a measure of statistical dispersion developed by the Italian statistician and sociologist Corrado Gini (the index was published in 1912).\n","\n","$$\n","Error({\\cal D}) = Gini({\\cal D}) = 1-\\sum\\limits_i p_i^2\n","$$\n","\n","  - Maximum: $(1 - 1/m)$ when records are equally distributed among all classes, implying least interesting information\n","  - Minimum: (0.0) when all records belong to one class, implying most interesting information"]},{"cell_type":"markdown","metadata":{"id":"Hlz541fAHCWr"},"source":["## Let's make an example\n","\n","- Suppose $|{\\cal D}|$ has 400 instances in class 0 and 400 instances in class 1, denote with ${\\cal D}=(400,400)$\n","   - $Error({\\cal D}) = 1 - (1/2)^2 - (1/2)^2 = 0.5$\n","   \n","\n","- Suppose Splitting $A=(f_1, t_1)$ produces ${\\cal D}_L=(300,100)$ and ${\\cal D}_R=(100,300)$:\n","   - $Gain(A|~{\\cal D}) = 0.5 - 400/800 * ( 1 - (3/4)^2 - (1/4)^2) - 400/800 *(1- (1/4)^2 - (3/4)^2) = 0.125$\n","   \n","   \n","- Suppose Splitting $B=(f_2, t_2)$ produces ${\\cal D}_L=(200,400)$ and ${\\cal D}_R=(200,000)$:\n","   - $Gain(B|~{\\cal D}) =  0.5 - 600/800 * ( 1 - (1/3)^2 - (2/3)^2) - 200/800 *(1- (1)^2 - (0)^2) \\approx 0.167$\n","   \n","   \n","- Largest Gain is for split B!"]},{"cell_type":"markdown","metadata":{"id":"H9HOH1vKHCWr"},"source":["## Exercise: Compare gini vs. information gain on a train/test split\n","\n","\n","\n","See documentation for the `criterion` parameter:\n","  - https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier"]},{"cell_type":"markdown","metadata":{"id":"g0fMPjkNHCWr"},"source":["# Decision tree on multiple classes\n","\n","Try a different number of clusters and leaves"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UY4L3k6gHCWr"},"outputs":[],"source":["from sklearn.datasets import make_blobs\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.inspection import DecisionBoundaryDisplay\n","\n","# create a (quasi)random dataset\n","N_SAMPLES = 1000\n","N_CENTERS = 6\n","X, y = make_blobs(n_samples=N_SAMPLES,\n","                  centers=N_CENTERS,\n","                  cluster_std=0.5)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","\n","# train and predict\n","dt = DecisionTreeClassifier(max_leaf_nodes=12) # change this!\n","dt.fit(X_train,y_train)\n","\n","# compute Accuracy\n","train_acc = accuracy_score(y_true = y_train, y_pred = dt.predict(X_train))\n","test_acc  = accuracy_score(y_true = y_test,  y_pred = dt.predict(X_test))\n","print (\"Train Accuracy: {:.3f} - Test Accuracy: {:.3f}\".format(train_acc,test_acc) )\n","\n","#model_decision_boundary(dt, X_test, y_test)\n","\n","fig, ax = plt.subplots(figsize=(4,4))\n","disp = DecisionBoundaryDisplay.from_estimator(\n","    dt, X_train,\n","    grid_resolution = 30, plot_method='pcolormesh',\n","    alpha=0.3, ax = ax)\n","\n","ax.scatter(X_test[:,0], X_test[:,1], c=y_test, edgecolor=\"k\", alpha=0.5);\n"]},{"cell_type":"markdown","metadata":{"id":"SIl__g0NHCWr"},"source":["## Stopping Criteria: max leaf nodes\n","\n","The number of leaves affects the model expressiveness."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LQp6OIBaHCWs"},"outputs":[],"source":["from sklearn.datasets import make_moons\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","\n","X, y = make_moons(n_samples=1000, noise=0.1)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","\n","# train and predict\n","dt = DecisionTreeClassifier(max_leaf_nodes=2) # change this!\n","dt.fit(X_train,y_train)\n","\n","fig, ax = plt.subplots(figsize=(4,4))\n","disp = DecisionBoundaryDisplay.from_estimator(\n","    dt, X_train,\n","    grid_resolution = 30, plot_method='pcolormesh',\n","    alpha=0.3, ax = ax)\n","\n","ax.scatter(X_test[:,0], X_test[:,1], c=y_test, edgecolor=\"k\", alpha=0.5);\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6M_9k6w8HCWs"},"outputs":[],"source":["from sklearn.datasets import make_moons\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","X, y = make_moons(n_samples=1000, noise=0.1)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","\n","for max_leaves in range(2,50):\n","    # train and predict\n","    dt = DecisionTreeClassifier(max_leaf_nodes=max_leaves)\n","    dt.fit(X_train,y_train)\n","\n","    # compute Accuracy\n","    train_acc = accuracy_score(y_true = y_train, y_pred = dt.predict(X_train))\n","    test_acc  = accuracy_score(y_true = y_test,  y_pred = dt.predict(X_test))\n","    print (\"Train Accuracy: {:.3f} - Test Accuracy: {:.3f}\".format(train_acc,test_acc) )"]},{"cell_type":"markdown","metadata":{"id":"8NFA-JdKHCWs"},"source":["## Stopping Criteria: maximum depth\n","\n","The number of leaves affects the model expressiveness."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NIvOTNicHCWs"},"outputs":[],"source":["from sklearn import tree\n","from sklearn.metrics import accuracy_score\n","\n","X, y = make_moons(n_samples=1000, noise=0.1)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","\n","# train and predict\n","dt = tree.DecisionTreeClassifier(max_depth=2) # change this!\n","dt.fit(X_train,y_train)\n","\n","fig, ax = plt.subplots(figsize=(4,4))\n","DecisionBoundaryDisplay.from_estimator(\n","    dt, X_train,\n","    grid_resolution = 30, plot_method='pcolormesh',\n","    alpha=0.3, ax = ax)\n","\n","ax.scatter(X_test[:,0], X_test[:,1], c=y_test, edgecolor=\"k\", alpha=0.5);\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"byh_PAyoHCWs","scrolled":false},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn import tree\n","from sklearn.metrics import accuracy_score\n","\n","X, y = make_moons(n_samples=1000, noise=0.1)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","\n","for max_depth in range(1,20):\n","    # train and predict\n","    dt = tree.DecisionTreeClassifier(max_depth=max_depth)\n","    dt.fit(X_train,y_train)\n","\n","    # compute Accuracy\n","    train_acc = accuracy_score(y_true = y_train, y_pred = dt.predict(X_train))\n","    test_acc  = accuracy_score(y_true = y_test,  y_pred = dt.predict(X_test))\n","    print (\"Train Accuracy: {:.3f} - Test Accuracy: {:.3f}\".format(train_acc,test_acc) )"]},{"cell_type":"markdown","metadata":{"id":"vQtjHSLmHCWs"},"source":["## Tuning of the tree\n","\n","The power of the tree can be tuned with the following constraints:\n","\n","- **max_depth**: The maximum depth of the tree.\n","\n","- **min_samples_split**: The minimum number of samples required to split an internal node\n","\n","- **min_samples_leaf**: The minimum number of samples required to be at a leaf node.\n","\n","- **max_leaf_nodes**: The maximum number of trees.\n","\n","- **min_impurity_decrease**: The minimum gain for allowing a split.\n","\n","- **min_impurity_split**: The minim error for allowing a split.\n","\n","In conjunction with the above constraints most implementations implement a smarter growing strategy.\n","Indeed, if we are limited in the number of nodes, then growing order makes a difference.\n","In this case, it is useful to evaluate the gain provided by splitting all of the tree leaves, and\n","then split the leaf the provides the maximum gain."]},{"cell_type":"markdown","metadata":{"id":"oDTHr13THCWs"},"source":["## Algorithm Sketch\n","\n","Non-recursive, best split first\n","\n","<div class=\"alert alert-info\">\n","    \n","BuildTree(${\\cal D}$):\n","1. $Tree \\gets \\emptyset$\n","1. $(f \\leq t) \\gets $  best split of ${\\cal D}$\n","1. *Gain* $\\gets$ goodness of the split $(f \\leq t)$\n","1. $Queue \\gets \\langle gain,(f\\leq t),{\\cal D}\\rangle$\n","1. **While** $Queue \\neq \\emptyset$ and *no other stopping criterion is met*:\n","1. $\\quad$ $\\langle gain, (f \\leq t), {\\cal D}^*\\rangle \\gets Queue$.pop_max()\n","1. $\\quad$ Add node $(f \\leq t)$ to $Tree$ at the leaf corresponding to ${\\cal D}^*$\n","1. $\\quad$ *(Left Partition)*\n","1. $\\quad$ ${\\cal D}_L \\gets \\{x \\in {\\cal D} ~|~ x_f\\leq t\\}$\n","1. $\\quad$ $(f \\leq t) \\gets $  best split of ${\\cal D}_L$\n","1. $\\quad$ *Gain* $\\gets$ goodness of the split $(f \\leq t)$\n","1. $\\quad$ $Queue$.push( $\\langle gain,(f\\leq t),{\\cal D}_L\\rangle$ )\n","1. $\\quad$ *(Right Partition)*\n","1. $\\quad$ ${\\cal D}_R \\gets \\{x \\in {\\cal D} ~|~ x_f> t\\}$\n","1. $\\quad$ $(f \\leq t) \\gets $  best split of ${\\cal D}_R$\n","1. $\\quad$ *Gain* $\\gets$ goodness of the split $(f \\leq t)$\n","1. $\\quad$ $Queue$.push( $\\langle gain,(f\\leq t),{\\cal D}_R\\rangle$ )\n","1. **Return** $Tree$\n","\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"igRAta7tHCWt"},"source":["## Regression\n","\n","\n","- Decision trees can be used also for regression problems.\n","\n","\n","- We must use the proper quality/cost function.\n","\n","\n","- For regression, the cost function is Mean Squared Error(MSE):\n","\n","$$\n","Error(tree, {\\cal D}) = MSE(tree, {\\cal D}) =  \\frac{1}{|{\\cal D}|} \\sum\\limits_{(x,y)\\in {\\cal D}} (tree(x)- y)^2\n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"jvpCwhvTHCWt"},"source":["## Leaf Node\n","\n","- Given a dataset ${\\cal D}$ what is the best prediction we can have?\n","\n","- $\\mu = \\arg\\min MSE(\\mu, {\\cal D})\\quad \\Rightarrow\\quad \\mu =  \\frac{1}{|{\\cal D}|} \\sum\\limits_{(x,y)\\in {\\cal D}} y$\n","\n","- for the regression task, it holds that $\\mu$ must be the average value of the labels in ${\\cal D}$\n","\n","- Given $\\mu$, we can write that total error on the dataset is:\n","$$\n","Error({\\cal D}) =  \\frac{1}{|{\\cal D}|} \\sum\\limits_{(x,y)\\in {\\cal D}} (\\mu - y)^2\n","$$\n","\n","- Hereinafter we denote with $Error({\\cal D})$ the error of the best prediction for dataset ${\\cal D}$.\n"]},{"cell_type":"markdown","metadata":{"id":"ivCg9rLrHCWt"},"source":["## Internal Node\n","\n","- Given the pair $f \\leq t$, we must determine the quality of this split.\n","\n","- In general, assuming $Error$ is an average measure, we denote the gain of a split as the error reduction w.r.t. to not splitting the node.\n","\n","$$\n","Gain(f,t~|~{\\cal D}) = Error({\\cal D}) - \\frac{|{\\cal D}_L|}{|{\\cal D}|} Error({\\cal D}_L) - \\frac{|{\\cal D}_R|}{|{\\cal D}|} Error({\\cal D}_R)\n","$$\n","\n","- We would like Gain>0, note that Gain cannot decrease.\n"]},{"cell_type":"markdown","metadata":{"id":"UG-hM3ChHCWu"},"source":["## Regression Problem"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jBhttZB8HCWu"},"outputs":[],"source":["from sklearn.datasets import load_boston\n","data = load_boston()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cj0FlYhQHCWu","scrolled":false},"outputs":[],"source":["print (data['DESCR'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_P9GdRtdHCWu"},"outputs":[],"source":["X = data.data\n","y = data.target\n","\n","print (X.shape)\n","print (y.shape)\n","print (data.feature_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"61HNUboIHCWu"},"outputs":[],"source":["%matplotlib inline\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.metrics import mean_squared_error\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n","\n","errors = []\n","\n","for max_leaves in range(2,100):\n","    # train and predict\n","    dt = DecisionTreeRegressor(max_leaf_nodes=max_leaves)\n","    dt.fit(X_train,y_train)\n","\n","    # compute Accuracy\n","    train_acc = mean_squared_error(y_true=y_train, y_pred=dt.predict(X_train))\n","    test_acc  = mean_squared_error(y_true=y_test,  y_pred=dt.predict(X_test))\n","\n","    errors += [ [max_leaves, train_acc, test_acc] ]\n","\n","errors = np.array(errors)\n","\n","fig, ax = plt.subplots()\n","ax.plot(errors[:,0], errors[:,1], \"x:\", label=\"Train\")\n","ax.plot(errors[:,0], errors[:,2], \"o-\", label=\"Test\")\n","ax.set_ylabel(\"MSE\")\n","ax.set_xlabel(\"Number of Leaves\")\n","ax.grid()\n","ax.legend();\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zk6icz2jHCWu"},"outputs":[],"source":["# Train a linear regression\n","\n","from sklearn.linear_model import LinearRegression\n","reg = LinearRegression()\n","reg.fit(X_train,y_train)\n","\n","reg_train_err = mean_squared_error(y_true=y_train, y_pred=reg.predict(X_train))\n","reg_test_err  = mean_squared_error(y_true=y_test,  y_pred=reg.predict(X_test))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r3y7LWMNHCWv"},"outputs":[],"source":["# Comparison plot\n","\n","fig, ax = plt.subplots()\n","ax.plot(errors[:,0], errors[:,1], \"x:\", label=\"DT Train\")\n","ax.plot(errors[:,0], errors[:,2], \"o-\", label=\"DT Test\")\n","ax.axhline(y=reg_train_err, c='red', ls=\"--\", label=\"Reg Train\")\n","ax.axhline(y=reg_test_err, c='green', ls=\"--\", label=\"Reg Test\")\n","ax.set_ylabel(\"MSE\")\n","ax.grid()\n","ax.legend();"]},{"cell_type":"markdown","metadata":{"id":"hSiw9AVhHCWv"},"source":["## Model Overfitting\n","\n","Breast_Cancer dataset: https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pln4XJALHCWv"},"outputs":[],"source":["from sklearn.datasets import load_breast_cancer\n","X, y = load_breast_cancer(return_X_y=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k00k-xSpHCWv"},"outputs":[],"source":["X.shape, y.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xckSBEnvHCWv"},"outputs":[],"source":["import numpy as np\n","uniq_vals, counts = np.unique(y, return_counts=True)\n","uniq_vals, counts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DpUl7zj3HCWv"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n","\n","for max_leaves in range(2,20):\n","    # train and predict\n","    dt = DecisionTreeClassifier(max_leaf_nodes=max_leaves)\n","    dt.fit(X_train,y_train)\n","\n","    # compute Accuracy\n","    train_acc = accuracy_score(y_true = y_train, y_pred = dt.predict(X_train))\n","    test_acc  = accuracy_score(y_true = y_test,  y_pred = dt.predict(X_test))\n","    print ( f\"Num. Leaves {max_leaves:2d} - \"\n","            f\"Train Accuracy: {train_acc:.3f} - Test Accuracy: {test_acc:.3f}\" )"]},{"cell_type":"markdown","metadata":{"id":"nKg9vAMpHCWv"},"source":["### Reasons for model overfitting\n","\n"," 1. Low data quality\n","    - presence of noise\n","    - lack of representative samples\n"," 2. High model complexity\n","    - multiple comparison procedure\n"]},{"cell_type":"markdown","metadata":{"id":"XipqCfrhHCWw"},"source":["### Multiple comparison procedure\n","\n","Consider the task of predicting whether stock market will rise/fall in the next 10 trading days\n","\n","Assume that with random guessing:\n","$$P(correct) = 0.5$$\n","\n","Make 10 random guesses in a row:\n","$$P(\\#correct \\geq 8 ) = \\frac{{10 \\choose 8} + {10 \\choose 9} + {10 \\choose 10}}{2^{10}} \\approx 0.05$$\n","\n","Approach:\n"," - take 50 analysts\n"," - each makes 10 random guesses\n"," - select the most accurate analyst\n","\n","Probability that at least one analyst makes 8 or more correct predictions:\n","$$P(\\#correct \\geq 8 ) = 1-(1-0.05)^{50} \\approx 0.94$$\n","\n","Most algorithms perform training steps (e.g., adding a node to a tree) by selecting the among several alternatives (e.g., feature-threshold pairs). The performance we see might be due to random guessing.\n","\n","Random guessing is not going to work on the test set!\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"w7qO9bWgHCWw"},"source":["### Model Selection\n","\n","How to choose among different models?\n","\n"," - Estimate the error on the test set (generalization error)\n"," - using the error on the training set is an optimistic measure\n"," - wait for it\n","\n","**Intuition: the model does not need to be overly complex**.\n","\n","**Occam's Razor**: Given two models with the same generalization errors, the simpler model is preferred over the more complex model.\n","\n","**Pre-pruning vs. post-pruning** methods:\n"," - pre-pruning: early stopping criteria (we have already seen a few)\n"," - post-pruning: grow completely then prune sub-trees\n","\n","Post allows to take decision by \"analyzing a larger pool of nodes\".\n","\n","**Intuition: include the model complexity in the cost being optimized**.\n","\n","The error of a model ${\\cal M}$ (e.g., a decision tree) is:\n","\n","$$Error({\\cal D}, {\\cal M})= \\frac{1}{|{\\cal D}|} \\sum\\limits_{(x,y) \\in {\\cal D}} E(y, {\\cal M}(x))$$\n","\n","The generalization error can be estimated as:\n","\n","$$ Error_{gen}({\\cal D}, {\\cal M}) = Error({\\cal D}, {\\cal M}) + \\alpha\\cdot Complexity ({\\cal M})$$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zUkfwhiIHCWw"},"source":["One common implementation is to measure the model complexity as the number of leaf nodes divided by the size of ${\\cal D}$.\n","\n","$$ Error_{gen}({\\cal D}, {\\cal M}) = Error({\\cal D}, {\\cal M}) + \\alpha\\cdot \\frac{\\#leaves\\ in\\ |{\\cal M}|}{|{\\cal D}|}$$\n","\n","With $\\alpha=1$ this means that adding a leaf costs $1/|{\\cal D}|$\n"," - adding a leaf is not convenient if $Error({\\cal D}, {\\cal M})$ is not reduced ($Error_{gen}$ increases)\n"," - adding a leaf is not convenient if $Error({\\cal D}, {\\cal M})$ is reduced by one instance ($Error_{gen}$ does not change)\n"," - adding a leaf is convenient if $Error({\\cal D}, {\\cal M})$ is reduced by more than one instance ($Error_{gen}$ decreases)"]},{"cell_type":"markdown","metadata":{"id":"70CsWrEbHCWw"},"source":["### Scikit-learn implementation (Breiman at. al)\n","\n","see https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html\n","\n","$$ Error_{gen}({\\cal D}, T) = Error({\\cal D}, T) + \\alpha\\cdot |L(T)|$$\n","\n"," - where $T$ is a decision tree, and $|L(T)|$ is the number of leaves it includes.\n","\n","which we can write on the basis on the leaves' errors:\n","$$ Error_{gen}({\\cal D}, T) = \\sum_{l \\in L(T)} Error({\\cal D}_{|l}, l) + \\alpha\\cdot |L(T)|$$\n","\n","- ${\\cal D}_{|l}$ denote the instances falling into $l$.\n","\n","\n","Similarly, if $t$ is a subtree of $T$, in general it holds that:\n","$$ Error_{gen}({\\cal D}_{|t}, t) = \\sum_{l \\in L(t)} Error({\\cal D}_{|l}, l) + \\alpha\\cdot |L(t)|$$\n","\n","If we replace the subtree $t$ with its root $r$, the corresponding generalization error would be:\n","\n","$$ Error_{gen}({\\cal D}_{|t}, r) = Error({\\cal D}_{|t}, r) + \\alpha$$\n","\n","\n","We ask when $t$ is more useful than $n$, i.e., when \n","\n","$$ Error_{gen}({\\cal D}_{|t}, t) <  Error_{gen}({\\cal D}_{|t}, r)$$\n","\n","In general it holds that:\n","$$ Error({\\cal D}_{|t}, t) <  Error({\\cal D}_{|t}, r)$$\n","\n","**Example**: if they are equal, it means that the sub-tree $t$ adds complexity to the model without reducing the error on the training set. Therefore we should remove $t$!\n","\n","But when adopting $Error_{gen}$, this depends on $\\alpha$.\n","\n","\n","We call effective alpha $\\alpha_{eff}(t)$ of a subtree $t$ the value of $\\alpha$ such that the two error above are equal:\n","\n","$$ Error_{gen}({\\cal D}_{|t}, t)\\ =  Error_{gen}({\\cal D}_{|t}, r)$$\n","$$ \\sum_{l \\in L(t)} Error({\\cal D}_{|l}, l) + \\alpha\\cdot |L(t)|\\ =  Error({\\cal D}_{|t}, r) + \\alpha$$\n","\n"," - when $\\alpha_{eff}(t)$ is small, the gain is limited\n"," - when $\\alpha_{eff}(t)$ is large, the gain is significant\n","\n","**Algorithm**:\n"," - grow a full/large tree.\n"," - sort subtrees by increasing $\\alpha_{eff}(t)$.\n"," - prune all sub-trees $t$ for which $\\alpha_{eff}(t)$ is smaller than a user defined threshold.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5lU8NyMIHCWw"},"outputs":[],"source":["dt = DecisionTreeClassifier(random_state=0)\n","path = dt.cost_complexity_pruning_path(X_train, y_train)\n","ccp_alphas, impurities = path.ccp_alphas, path.impurities\n","\n","fig, ax = plt.subplots()\n","ax.plot(ccp_alphas[:-1], impurities[:-1], marker=\"o\", drawstyle=\"steps-post\")\n","ax.set_xlabel(\"effective alpha\")\n","ax.set_ylabel(\"total impurity of leaves\")\n","ax.set_title(\"Total Impurity vs effective alpha for training set\");"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0qsp6LGkHCWw"},"outputs":[],"source":["# materialize all possible trees\n","ccp_alphas = ccp_alphas[:-1]\n","dts = []\n","for ccp_alpha in ccp_alphas:\n","    dt = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n","    dt.fit(X_train, y_train)\n","    dts.append(dt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gGYIJ8pEHCWx"},"outputs":[],"source":["node_counts = [dt.tree_.node_count for dt in dts]\n","depth = [dt.tree_.max_depth for dt in dts]\n","fig, ax = plt.subplots(2, 1)\n","ax[0].plot(ccp_alphas, node_counts, marker=\"o\", drawstyle=\"steps-post\")\n","ax[0].set_xlabel(\"alpha\")\n","ax[0].set_ylabel(\"number of nodes\")\n","ax[0].set_title(\"Number of nodes vs alpha\")\n","ax[1].plot(ccp_alphas, depth, marker=\"o\", drawstyle=\"steps-post\")\n","ax[1].set_xlabel(\"alpha\")\n","ax[1].set_ylabel(\"depth of tree\")\n","ax[1].set_title(\"Depth vs alpha\")\n","fig.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ciMF8R0zHCWx"},"outputs":[],"source":["train_scores = [dt.score(X_train, y_train) for dt in dts]\n","test_scores = [dt.score(X_test, y_test) for dt in dts]\n","\n","fig, ax = plt.subplots()\n","ax.set_xlabel(\"alpha\")\n","ax.set_ylabel(\"accuracy\")\n","ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n","ax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\n","ax.plot(ccp_alphas, test_scores, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\n","ax.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xt3nQzn8HCWx"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lSvQbvAQHCWx"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fPwoLdgTHCWx"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ucxmEMcrHCWx"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"-Z3T_QtkHCWx"},"source":["## Decision Trees Implementation"]},{"cell_type":"markdown","metadata":{"id":"Vt42Iv-0HCWx"},"source":["## Algorithm Sketch\n","\n","Non-recursive, best split first\n","\n","<div class=\"alert alert-info\">\n","    \n","BuildTree(${\\cal D}$):\n","1. $Tree \\gets \\emptyset$\n","1. $(f \\leq t) \\gets $  best split of ${\\cal D}$\n","1. *Gain* $\\gets$ goodness of the split $(f \\leq t)$\n","1. $Queue \\gets \\langle gain,(f\\leq t),{\\cal D}\\rangle$\n","1. **While** $Queue \\neq \\emptyset$ and *no other stopping criterion is met*:\n","1. $\\quad$ $\\langle gain, (f \\leq t), {\\cal D}^*\\rangle \\gets Queue$.pop_max()\n","1. $\\quad$ Add node $(f \\leq t)$ to $Tree$ at the leaf corresponding to ${\\cal D}^*$\n","1. $\\quad$ *(Left Partition)*\n","1. $\\quad$ ${\\cal D}_L \\gets \\{x \\in {\\cal D} ~|~ x_f\\leq t\\}$\n","1. $\\quad$ $(f \\leq t) \\gets $  best split of ${\\cal D}_L$\n","1. $\\quad$ *Gain* $\\gets$ goodness of the split $(f \\leq t)$\n","1. $\\quad$ $Queue$.push( $\\langle gain,(f\\leq t),{\\cal D}_L\\rangle$ )\n","1. $\\quad$ *(Right Partition)*\n","1. $\\quad$ ${\\cal D}_R \\gets \\{x \\in {\\cal D} ~|~ x_f> t\\}$\n","1. $\\quad$ $(f \\leq t) \\gets $  best split of ${\\cal D}_R$\n","1. $\\quad$ *Gain* $\\gets$ goodness of the split $(f \\leq t)$\n","1. $\\quad$ $Queue$.push( $\\langle gain,(f\\leq t),{\\cal D}_R\\rangle$ )\n","1. **Return** $Tree$\n","\n","</div>\n","\n","\n","See for instance https://github.com/microsoft/LightGBM, https://github.com/dmlc/xgboost, https://github.com/catboost/catboost."]},{"cell_type":"markdown","metadata":{"id":"HG9siselHCWy"},"source":["## Driving Factor\n","\n","We let the design of our algorithm be driven by the quality measure adopted.\n","\n","For classification, we can adopt **error** $E$, that is the fraction of misclassified instances."]},{"cell_type":"markdown","metadata":{"id":"8J08s0_SHCWy"},"source":["## Regression\n","\n","\n","- Decision trees can be used also for regression problems.\n","\n","\n","- We must use the proper quality/cost function.\n","\n","\n","- For regression, the cost function is Mean Squared Error(MSE):\n","\n","$$\n","Error(tree, {\\cal D}) = MSE(tree, {\\cal D}) =  \\frac{1}{|{\\cal D}|} \\sum\\limits_{(x,y)\\in {\\cal D}} (tree(x)- y)^2\n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"fqWz6I60HCWy"},"source":["## Leaf Node\n","\n","- Given a dataset ${\\cal D}$ what is the best prediction we can have?\n","\n","- $\\mu = \\arg\\min MSE(\\mu, {\\cal D})\\quad \\Rightarrow\\quad \\mu =  \\frac{1}{|{\\cal D}|} \\sum\\limits_{(x,y)\\in {\\cal D}} y$\n","\n","- for the regression task, it holds that $\\mu$ must be the average value of the labels in ${\\cal D}$\n","\n","- Given $\\mu$, we can write that total error on the dataset is:\n","$$\n","Error({\\cal D}) =  \\frac{1}{|{\\cal D}|} \\sum\\limits_{(x,y)\\in {\\cal D}} (\\mu - y)^2\n","$$\n","\n","- Hereinafter we denote with $Error({\\cal D})$ the error of the best prediction for dataset ${\\cal D}$.\n"]},{"cell_type":"markdown","metadata":{"id":"aWj-gaDpHCWy"},"source":["## Internal Node\n","\n","- Given the pair $f \\leq t$, we must determine the quality of this split.\n","\n","- In general, assuming $Error$ is an average measure, we denote the gain of a split as the error reduction w.r.t. to not splitting the node.\n","\n","$$\n","Gain(f,t~|~{\\cal D}) = Error({\\cal D}) - \\frac{|{\\cal D}_L|}{|{\\cal D}|} Error({\\cal D}_L) - \\frac{|{\\cal D}_R|}{|{\\cal D}|} Error({\\cal D}_R)\n","$$\n","\n","- We would like Gain>0, note that Gain cannot decrease.\n"]},{"cell_type":"markdown","metadata":{"id":"u9WrwMZPHCWy"},"source":["$$\n","Gain(f,t~|~{\\cal D}) =\n","     \\frac{1}{|{\\cal D}|} \\sum\\limits_{(x,y)\\in {\\cal D}} (\\mu - y)^2 -\n","     \\frac{|{\\cal D}_L|}{|{\\cal D}|} \\frac{1}{|{\\cal D}_L|} \\sum\\limits_{(x,y)\\in {\\cal D}_L} (\\mu_L - y)^2  -\n","     \\frac{|{\\cal D}_R|}{|{\\cal D}|} \\frac{1}{|{\\cal D}_R|} \\sum\\limits_{(x,y)\\in {\\cal D}_R} (\\mu_R - y)^2  \n","$$"]},{"cell_type":"markdown","metadata":{"id":"WhrFGqndHCWy"},"source":["simplify"]},{"cell_type":"markdown","metadata":{"id":"pqrOS3oRHCWy"},"source":["$$\n","Gain(f,t~|~{\\cal D}) = \\frac{1}{|{\\cal D}|} \\left(\n","     \\sum\\limits_{(x,y)\\in {\\cal D}} (\\mu - y)^2 -\n","     \\sum\\limits_{(x,y)\\in {\\cal D}_L} (\\mu_L - y)^2  -\n","     \\sum\\limits_{(x,y)\\in {\\cal D}_R} (\\mu_R - y)^2  \n","     \\right)\n","$$"]},{"cell_type":"markdown","metadata":{"id":"EzWs3rZ6HCWy"},"source":["compute squares"]},{"cell_type":"markdown","metadata":{"id":"ZNngn-cRHCWz"},"source":["$$\n","Gain(f,t~|~{\\cal D}) = \\frac{1}{|{\\cal D}|} \\left(\n","     \\sum\\limits_{(x,y)\\in {\\cal D}} (\\mu^2 + y^2 - 2y\\mu) -\n","     \\sum\\limits_{(x,y)\\in {\\cal D}_L} (\\mu_L^2 + y^2 - 2y\\mu_L)  -\n","     \\sum\\limits_{(x,y)\\in {\\cal D}_R}(\\mu_R^2 + y^2 - 2y\\mu_R)\n","     \\right)\n","$$"]},{"cell_type":"markdown","metadata":{"id":"kRsuVH0dHCWz"},"source":["simplify $y^2$"]},{"cell_type":"markdown","metadata":{"id":"bL0A9rtHHCWz"},"source":["$$\n","Gain(f,t~|~{\\cal D}) = \\frac{1}{|{\\cal D}|} \\left(\n","     \\sum\\limits_{(x,y)\\in {\\cal D}} (\\mu^2 - 2y\\mu) -\n","     \\sum\\limits_{(x,y)\\in {\\cal D}_L} (\\mu_L^2 - 2y\\mu_L)  -\n","     \\sum\\limits_{(x,y)\\in {\\cal D}_R}(\\mu_R^2 - 2y\\mu_R)\n","     \\right)\n","$$"]},{"cell_type":"markdown","metadata":{"id":"vNsKsYDPHCWz"},"source":["compute sums:"]},{"cell_type":"markdown","metadata":{"id":"s0GRFOuRHCWz"},"source":["$$\n","Gain(f,t~|~{\\cal D}) = \\frac{1}{|{\\cal D}|} \\left(\n","     |{\\cal D}|\\mu^2 - 2\\mu \\sum\\limits_{(x,y)\\in {\\cal D}} y \\quad -\n","     |{\\cal D}_L|\\mu_L^2 + 2\\mu_L \\sum\\limits_{(x,y)\\in {\\cal D}_L} y \\quad  -\n","     |{\\cal D}_R|\\mu_R^2 + 2\\mu_R \\sum\\limits_{(x,y)\\in {\\cal D}_R} y\n","     \\right)\n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"sHVj7dStHCWz"},"source":["rewrite sum as mean multiplied by the number of elements"]},{"cell_type":"markdown","metadata":{"id":"iEzI2ogZHCWz"},"source":["$$\n","Gain(f,t~|~{\\cal D}) = \\frac{1}{|{\\cal D}|} \\left(\n","     |{\\cal D}|\\mu^2 - 2\\mu  |{\\cal D}|\\mu \\quad -\n","     |{\\cal D}_L|\\mu_L^2 + 2\\mu_L |{\\cal D}_L| \\mu_L\\quad  -\n","     |{\\cal D}_R|\\mu_R^2 + 2\\mu_R  |{\\cal D}_R| \\mu_R\n","     \\right)\n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"U-IhZlkFHCW0"},"source":["$$\n","Gain(f,t~|~{\\cal D}) = \\frac{1}{|{\\cal D}|} \\left(\n","     - |{\\cal D}|\\mu^2 + |{\\cal D}_L|\\mu_L^2 + |{\\cal D}_R|\\mu_R^2\n","     \\right)\n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"yG82PNthHCW0"},"source":["if we are interested in maximizing the Gain rather than computing its exact value, we can simplify multiplicative factors and constants independent of $f$ and $t$"]},{"cell_type":"markdown","metadata":{"id":"fUyKGM-NHCW0"},"source":["$$\n","Gain(f,t~|~{\\cal D}) \\propto\n","    |{\\cal D}_L|\\mu_L^2 +|{\\cal D}_R|\\mu_R^2\n","$$"]},{"cell_type":"markdown","metadata":{"id":"rsqp9gHcHCW0"},"source":["$$\n","Gain(f,t~|~{\\cal D}) \\propto \\frac{\\left(\\sum_{(x,y)\\in {\\cal D}_L} y\\right)^2}{|{\\cal D}_L|}\n","+ \\frac{\\left(\\sum_{(x,y)\\in {\\cal D}_R} y\\right)^2}{|{\\cal D}_R|}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"2aEdvg5THCW0"},"source":["The best split is found by maximizing the above Gain."]},{"cell_type":"markdown","metadata":{"id":"CaiB7wwFHCW0"},"source":["## Implementation"]},{"cell_type":"markdown","metadata":{"id":"iz09noA5HCW0"},"source":["Given $f$, computing the Gain for each possible threshold $t$ can be done efficiently as follows."]},{"cell_type":"markdown","metadata":{"id":"Ht4ywov5HCW0"},"source":["Suppose we have the following single-feature dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xoU0c0C1HCW0"},"outputs":[],"source":["import numpy as np\n","\n","X_f = np.array( [1,3,6,4,2,7,9,9,0,1] )\n","y   = np.array( [1,1,1,2,2,3,3,3,1,1] )"]},{"cell_type":"markdown","metadata":{"id":"ZlOY92w7HCW1"},"source":["Feature values can be sorted beforehand. Below we just get the sorting indices."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lC0g7g7dHCW1"},"outputs":[],"source":["sorted_idx = np.argsort(X_f)\n","\n","# print (sorted_idx)\n","print ( X_f[sorted_idx] )\n","print ( y[sorted_idx])"]},{"cell_type":"markdown","metadata":{"id":"waHN8OkeHCW1"},"source":["At this point it is easy to identify ${\\cal D}_L$ and ${\\cal D}_R$ for every possible threshold.\n","\n","We can easily compute $\\sum_{(x,y)\\in {\\cal D}_L} y$ for every possible split as below.\n","\n","While $\\sum_{(x,y)\\in {\\cal D}_R} y$ can be computed by difference."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oybojd3PHCW1"},"outputs":[],"source":["sum_y_L = np.cumsum( y[sorted_idx] )\n","sum_y_R = sum_y_L[-1] - sum_y_L\n","print (sum_y_L)\n","print (sum_y_R)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"znBCMkwwHCW1"},"outputs":[],"source":["count_L = np.arange(1,len(y)+1)\n","count_R = len(y) - count_L\n","print (count_L)\n","print (count_R)"]},{"cell_type":"markdown","metadata":{"id":"FOySb7FnHCW1"},"source":["It is now easy to compute all the gains."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sbF84x5wHCW1"},"outputs":[],"source":["gains = sum_y_L**2/count_L + sum_y_R**2/count_R\n","print (gains)"]},{"cell_type":"markdown","metadata":{"id":"pvmA7VR_HCW1"},"source":["yes, the last point is not a real split.\n","\n","**Note** that gain does not have a regular behavior, and therefore we cannot apply smart search strategies."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3MTg3V74HCW1"},"outputs":[],"source":["best_split = np.argmax(gains[:-1])\n","print (\"best split pos = \", best_split)\n","print (\"best threhold = \", X_f[sorted_idx][best_split])\n","print (\"best split is X_f <=\", X_f[sorted_idx][best_split])"]},{"cell_type":"markdown","metadata":{"id":"IMcBMi2JHCW2"},"source":["Typically, to improve robustness to noise, the actual threshold used in the tree is computed as follows.\n","\n","As our goal is to separate ${\\cal D}_L$ from ${\\cal D}_R$, we can chose a threshold in the middle of the two."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wKFq7HfAHCW2"},"outputs":[],"source":["best_split_threshold = ( X_f[sorted_idx][best_split] + X_f[sorted_idx][best_split+1] ) / 2\n","print (\"best split is X_f <=\", best_split_threshold )"]},{"cell_type":"markdown","metadata":{"id":"8uORYyJ4HCW2"},"source":["## References\n","\n","- **Introduction to Data Mining (Second Edition)**. , Kumar et al.\n","  - Chapter 4: Classification: Basic Concepts, Decision Trees, and Model Evaluation\n","    - until 4.5 included\n","  - Section 5.2: Nearest-Neighbor classifiers\n","\n","\n","- **Python Data Science Handbook**. O’Reilly. 2016\n","  - Chapter 5: Machine Learning - Introducing Scikit-Learn\n","  \n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.8.5 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"},"vscode":{"interpreter":{"hash":"e86f0528a1b45e531703c752e90d2c936b2dda6dac6764f4cd4f9141650544b5"}}},"nbformat":4,"nbformat_minor":0}
